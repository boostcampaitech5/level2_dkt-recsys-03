# epoch for training
epoch: 1

# optimizer
optimizer: adam

# learning rate
lr: 0.0001

# log steps
steps: 1