# epoch for training
epoch: 1

# optimizer
optimizer: adam

# learning rate
lr: 0.0001

# log steps
steps: 1

# patience for early stopping
patience : 1